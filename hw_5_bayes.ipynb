{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88fe2596-8e7c-4c97-b347-7468994508f8",
   "metadata": {},
   "source": [
    "# Домашнее задание №5 - SVM и Naive Bayes classifier\n",
    "\n",
    "В этом домашнем задании мы с вами напишем реализацию метода опорных векторов (Support Vector Machine), а также посмотрим на наивный байесовский классификатор из sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204ec7b-1f3c-439c-8a9e-0abbe270e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f9411-bcff-4fae-a4a7-3c42361de2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 12, 9\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "SEED = 111\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc2a8f3",
   "metadata": {},
   "source": [
    "## Задание 1. SVM своими руками\n",
    "\n",
    "**70 баллов**\n",
    "\n",
    "Идея метода опорных векторов сотоит в нахождении гиперплоскости (в общем нелинейном случае гиперповерхности) в пространстве фичей, оптимально разделяющей заданные объекты с **максимальным** зазором между объектами разных классов. \n",
    "\n",
    "Начнем с простого случая: пространство из двух фичей, а гиперплоскость - линейная функция.\n",
    "\n",
    "### Задание 1.1 Линейный SVM (40 баллов)\n",
    "\n",
    "Сгенерируем \"игрушечные\" данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c83aff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_blobs(n_samples=500, centers=2, random_state=SEED)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\")\n",
    "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9da672",
   "metadata": {},
   "source": [
    "Напишем класс модели `Linear_SVM`, которая подбирает оптимальную разделяющую поверхность (в 2D прямую) для наших данных.\n",
    "\n",
    "Какой же лосс использовать, чтобы оптимизировать нашу модель? Для SVM (SCV) используют Hinge loss (шарнирная функция потерь). Выглядит она следующим образом:\n",
    "\n",
    "$$\n",
    "  J(w, b) = λ \\frac{1}{2} ||w||^2 + \\frac{1}{n} \\sum \\limits _{i=1} ^{n} max(0, 1 - y_{i}(w x_{i} + b))\n",
    "$$\n",
    "\n",
    "\n",
    "Где w и b - веса и смещения нашей линейной функции. Что мы тут видим:\n",
    "\n",
    "- Первый член в основном отвечает за максимизацию маржи, выраженной как задача минимизации с добавленным параметром регуляризации λ. Умножение на 1/2 производится исключительно для удобства при вычислении градиента функции.\n",
    "\n",
    "\n",
    "- Вообще говоря, Hinge Loss это именно второе слогаемое функции выше. Этот член отвечает за то, чтобы мы прогнозировали правильную метку класса с достаточным запасом. Например, если yᵢ = 1 и xᵢ классифицирован правильно, вычисление лосса даст ноль, поскольку max(0, 1–1) = 0. Однако, если метка класса предсказана неверно, лосс даст значение больше нуля.\n",
    "\n",
    "Мы будем использовать градиентный спуск для оптимизации. Поэтому нам также нужно вычислить производные этой функции. Не будем мучаться больше чем необходимо, ответ приведен ниже. Лосс принимает две формы взависимости от величины линейной функции:\n",
    "\n",
    "$$J_{i} = λ \\frac{1}{2} ||w||^2$$\n",
    "\n",
    "если $$y_{i}(w x_{i} + b) >= 1$$ и в противном случае\n",
    "\n",
    "$$\n",
    "J_{i} = λ \\frac{1}{2} ||w||^2 + 1 - y_{i}(w x_{i} + b) \n",
    "$$ \n",
    "\n",
    "Для этих случаев происзводные принимают вид:\n",
    "\n",
    "$$\n",
    "  \\frac {\\partial J_{i}}{\\partial w_{k}} = λ w_{k}\n",
    "$$\n",
    "\n",
    "$$\n",
    "   \\frac { \\partial J_{i}}{ \\partial b} = 0\n",
    "$$\n",
    "\n",
    "и\n",
    "\n",
    "$$\n",
    "  \\frac {\\partial J_{i}}{\\partial w_{k}} = λ w_{k} - y_{i} x_{i}\n",
    "$$\n",
    "$$\n",
    "   \\frac { \\partial J_{i}}{ \\partial b} = - y_{i}\n",
    "$$\n",
    "\n",
    "соответственно.\n",
    "\n",
    "Перейдем к реализации нашего класса. Вам надо заполнить пропуски в методах `fit`, `estimate` и `predict`, и `Hinge_loss`. \n",
    "\n",
    "Для начала реализуйте `estimate` и `predict` основываясь на подсказках в методе. Будьте внимательны относительно типа который возвращают методы.\n",
    "\n",
    "Затем реализуйте метод `Hinge_loss`. Вам надо воспроизвести формулу выше. Обратите внимание на тип аргументов, принимаемых функцией. Проверьте свою реализацию ниже при помощи assert на паре примеров. Cравните с [функцией из `sklearn`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html)\n",
    "\n",
    "Теперь перейдем к `fit`. Алгоритм следующий:\n",
    "\n",
    "1. Инициализируем весов и смещений\n",
    "\n",
    "\n",
    "2. Отображаем метки классов из {0, 1} в {-1, 1}\n",
    "\n",
    "\n",
    "3. Выполняем градиентный спуск `n_iters` итераций. D каждую итерацию проходимся по всем объектам. Для каждого вычисляем градиенты и проводим обновление весов и смещений. На каждой итерации также запоминаем значение функции потерть. Добавим также аргумент `verbose`, который который позволит следить за функцией потерь в ходе обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c2584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_SVM():\n",
    "    \"\"\"\n",
    "    Linear Support Vector Machine implementation\n",
    "    \n",
    "    Attributes\n",
    "    -----------\n",
    "    lr: float\n",
    "        learning rate\n",
    "        \n",
    "    lambda_param: float \n",
    "        lambda parameter for regularisation\n",
    "        \n",
    "    n_iters: int\n",
    "        numbers of gradient descent iterations\n",
    "        \n",
    "    w: np.array(float)\n",
    "        model weights\n",
    "    \n",
    "    b: float\n",
    "        intercept term in the model\n",
    "        \n",
    "    loss_train: np.array(float)\n",
    "        loss value obtained during fit\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr=0.001, lambda_param=0.01, n_iters=100):\n",
    "        self.lr = lr\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        \n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        \n",
    "        self.loss_train = None\n",
    "\n",
    "    def fit(self, X, y, verbose=False):\n",
    "        \"\"\"\n",
    "        Fit model\n",
    "        Assign model parameters w and b\n",
    "        \n",
    "        X: np.array(n_samples, n_features)\n",
    "            training data\n",
    "            \n",
    "        y: np.array(n_samples)\n",
    "            class labels, 0 and 1\n",
    "            \n",
    "        verbose: bool\n",
    "            verbose loss during fit\n",
    "            \n",
    "        Returns\n",
    "        ---------\n",
    "        self\n",
    "        \"\"\"\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # изменим кодировку таргетной переменной на -1, 1,\n",
    "        y_cls_map = np.where(y <= 0, -1, 1)\n",
    "        \n",
    "        # инициализируем веса и смещение\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "        \n",
    "        self.loss_train = np.zeros(self.n_iters) # oбнуляем лоссы\n",
    "        for n in range(self.n_iters):\n",
    "            \n",
    "            # в ходе одной итерации проходимся по всем \n",
    "            # объектам в трейне\n",
    "            for i, x_i in enumerate(X):\n",
    "                \n",
    "                # для каждого объекта проверяем правильность предсказания\n",
    "                estimate = self.estimate(x_i) \n",
    "                condition = (y_cls_map[i] * estimate) >= 1\n",
    "                \n",
    "                # вычисляем градиент\n",
    "                # если объект на верной стороне гиперплоскости\n",
    "                if condition:\n",
    "                    dw = #YOUR CODE HERE\n",
    "                    db = #YOUR CODE HERE\n",
    "                # если объект на неверной стороне гиперплоскости\n",
    "                else:\n",
    "                    dw = #YOUR CODE HERE\n",
    "                    db = #YOUR CODE HERE\n",
    "                \n",
    "                # обновляем веса\n",
    "                self.w -= self.lr * dw\n",
    "                self.b -= self.lr * db\n",
    "                \n",
    "                # рассчитываем и выводим лосс\n",
    "                loss = #YOUR CODE HERE\n",
    "                self.loss_train[n] = loss\n",
    "                if verbose:\n",
    "                    print(f\"--------------- Epoch {n + 1} --> Loss = {round(loss, 3)} ---------------\")\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def Hinge_loss(self, y, y_pred):\n",
    "        \"\"\"\n",
    "        Hinge loss\n",
    "        \n",
    "        y: np.array(int)\n",
    "            true labels, 0 and 1\n",
    "            \n",
    "        y_pred: np.array(int)\n",
    "            predicted labels, 0 and 1\n",
    "        \n",
    "        Returns\n",
    "        ---------\n",
    "        float, loss value\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def estimate(self, X):\n",
    "        \"\"\"\n",
    "        Вычисляем и возвращаем значение\n",
    "        линейной функции wX + b\n",
    "        \n",
    "        X: np.array(n_samples, n_features)\n",
    "            training data\n",
    "            \n",
    "        Returns\n",
    "        ---------\n",
    "        np.array(float)\n",
    "        \"\"\"\n",
    "        return # YOUR CODE HERE\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Предсказываем метки классов\n",
    "        \n",
    "        X: np.array(n_samples, n_features)\n",
    "            training data\n",
    "            \n",
    "         Returns\n",
    "        ---------\n",
    "        np.array(int) array of class labels as 0 and 1\n",
    "        \"\"\"\n",
    "        \n",
    "        # сначала вычисляем self.estimate(X) и конвертируем значения в метки классов\n",
    "        # не забудьте, что сначала мы определяем класс как -1/1 сравнивая с 0\n",
    "        # а потом трансформируем в 0/1\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe1abc3",
   "metadata": {},
   "source": [
    "Обучите свою модель на сгенерированных выше данных. Разбивать на трейн и тест не надо. Постройте график лосс функции. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630b80f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучаем модель\n",
    "lin_SVM = Linear_SVM()\n",
    "lin_SVM.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde8785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# строим график лосса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6633d2",
   "metadata": {},
   "source": [
    "Визуализируйте, как модель разделила данные, используя заготовленную ниже функцию `plot_decision_boundary`. Обратите внимание, что фурнкция строит границу раздела численно, а не аналитически, поэтому у вас будет немного ломанная линия, а не идеальная прямая, это нормально."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ac2686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, model):\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr')\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50))\n",
    "    xy = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "    Z = model.predict(xy).reshape(xx.shape)\n",
    "\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='bwr')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fa386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(X, y, lin_SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaf238c",
   "metadata": {},
   "source": [
    "Посмотрите на получившийся график, возьмите 5 произвольных точекс плоскости (не из Х, а на глаз), которые будут принадлежать к разным классам и используйте их как пример для предсказания:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62408c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример предсказания\n",
    "X_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8af6142",
   "metadata": {},
   "source": [
    "В реальных случаях возможность эффективно разделить оъекты линейной функцией - довольно редкий случай. Усложним наш игрушечный датасет. Проверьте, как модель `Linear_SVM` справится с этими данными. \n",
    "\n",
    "\n",
    "Обучите модель и постройте график лосса и границу раздела. Вычислите метрики класификации (accuracy, precision, recall, f1) на обучающей выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4d8162",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(n_samples=500, noise=0.30, random_state=SEED)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\")\n",
    "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba95945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучаем модель\n",
    "lin_SVM = Linear_SVM()\n",
    "lin_SVM.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d32f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(X, y, lin_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5c9c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# строим график лосса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc2c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "№"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070220ff",
   "metadata": {},
   "source": [
    "### Задание 1.2 SVM с нелинейным ядром (30 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4119e76",
   "metadata": {},
   "source": [
    "Чтобы найти нелинейную границу решения, мы можем добавить такой прием как «ядро» (kernel). Если данные не являются линейно разделимыми в исходном пространстве, то мы применяем преобразованиe к данным, которое отображает данные из исходного пространства в более многомерное пространство. Цель состоит в том, что после преобразования в более многомерное пространство классы становятся линейно разделимыми. Кажется не очевидно, но вот вам картинка с визуальным примером перехода из двумерного пространства в трехмерное:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202974bb",
   "metadata": {},
   "source": [
    "![](kernel_transformation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1919f15e",
   "metadata": {},
   "source": [
    "Таким образом наша линейная функция wX + b превращается вследующее:\n",
    "\n",
    "$$\n",
    "  \\sum \\limits _{i} α_{i} K(x_{i}, x) + b\n",
    "$$\n",
    "\n",
    "где K(xi, x) это kernel функция, описфвающая трансформацию наших данных. Давайте возьмем SVC из sklearn и посмотрим как разные ядра, справятся с разделением модельных данных. попробуйте 'rbf', 'poly' (посмотрите на разные степени) и 'sigmoid'. Для якаждого случая посчитайте метрики классификациии на данных использованных для обучения и постройте границу раздела. Опишите, что получилось.\n",
    "\n",
    "доп. 10 баллов, если оформите все графики в красивый мультиплот с несколькими колонками и рядами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69fc9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel= ???)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603c6a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваш ход"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916b7736-caf6-4c5d-920d-1d3d9c7b89ea",
   "metadata": {},
   "source": [
    "## Задание 2. Применение баесовского классификатора\n",
    "\n",
    "**80 баллов**\n",
    "\n",
    "Мы будем использовать данные по свойствам покемонов (https://www.kaggle.com/abcsds/pokemon). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444bc945",
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon = pd.read_csv(\"Pokemon.csv\")\n",
    "pokemon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d89ccc",
   "metadata": {},
   "source": [
    "Начинаем конечно же с EDA. (35 баллов) Посмотрите на данные, типы фичей, их описательные статистики и распределения. А также также посмотреть, как различные признаки связаны между собой и с целевой переменной `Legendary` (постройте хитмап корреляции между фичами и целевой переменной). \n",
    "\n",
    "Мы будем предсказывать является ли покемон легендарным или нет. Замените логическое значение колонки `Legendary` на числовое (перекодировав на 0 и 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af194ff0",
   "metadata": {},
   "source": [
    "**Вопрос: как в этом случае лучше закодировать категориальные признаки (может быть, лучше их просто выбросить?).**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db7a2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12e09fa0",
   "metadata": {},
   "source": [
    "Разделите ваши данные на тестовую и тренировочную выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68671bd-8082-4ac7-a159-13e8e8563049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "321d3f38",
   "metadata": {},
   "source": [
    "Обучите модель `GaussianNB` из `sklearn` (5 баллов). Побробнее про разные реализации NB в sklearn можно почитать [тут](https://scikit-learn.org/stable/modules/naive_bayes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c432c38a-7ebf-4b3c-bb87-60d49a0d5787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f70b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENTER YOUR CODE HERE (/¯◡ ‿ ◡)/¯☆*##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dc12fe",
   "metadata": {},
   "source": [
    "Поработаем с получившейся моделью:\n",
    "\n",
    "1. Посчитайте метрики классификации (7 баллов)\n",
    "\n",
    "2. Нарисуйте confusion matrix (8 баллов)\n",
    "\n",
    "3. Изобразите ROC кривую и посчитайте площадь под ней (10 баллов)\n",
    "\n",
    "4. Скажите, какие признаки оказались наиболее важны для модели? (10 баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f58f5a-3b46-4562-b4bb-6d09c9a5fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENTER YOUR CODE HERE (/¯◡ ‿ ◡)/¯☆*##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8441428d-5ff4-46e0-927f-47ee935b224d",
   "metadata": {},
   "source": [
    "## Therapy time\n",
    "\n",
    "Напишите здесь ваши впечатления о задании: было ли интересно, было ли слишком легко или наоборот сложно и тд. Также сюда можно написать свои идеи по улучшению заданий, а также предложить данные, на основе которых вы бы хотели построить следующие дз. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a2c8ee-96ac-4967-a9d5-55563826ee1d",
   "metadata": {},
   "source": [
    "**Ваши мысли:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c4208a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
